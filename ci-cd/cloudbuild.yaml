# - Airflow Env variables.
substitutions:
  #_COMPOSER_ENV_NAME: test-env   # Declared in GCP cloudbuild trigger
  #_DEV_ENV_NAME: test_env        # Declared in GCP cloudbuild trigger
  _GCP_PROJECT_ID: <gcp_project_id>  # << Add your project-id here
  _COMPOSER_REGION: us-central1
  _COMPOSER_BUILD_IMG_NAME: airflow2_cicd:latest
  _COMPOSER_VARS_FILE: composer-env-vars
  _COMPOSER_CONF_FILE: composer-config
  _DAG_TO_EMAIL: send_email
  #_AIRFLOW_VERSION: '2.1.1'      # Declared in docker/Dockerfile-CI
  _PYTHON_VERSION: '3.8'

steps:
# - Verify Composer's env name.
  # [START env name]
  - name: 'gcr.io/cloud-builders/docker'
    waitFor: ['-']
    entrypoint: 'bash'
    args:
      - '-c'
      - |
        echo "*******************"
        echo "$_COMPOSER_ENV_NAME"
        echo "*******************"
    id: 'env name'
  # [END env name]

# - Pull the existing Airflow build from GCR if any.
  # [START pull-airflow2]
  - name: 'gcr.io/cloud-builders/docker'
    waitFor: ['-']
    entrypoint: 'bash'
    args:
      - '-c'
      - |
        if [ "${_COMPOSER_ENV_NAME}" = "${_DEV_ENV_NAME}" ]
        then
          docker pull gcr.io/${PROJECT_ID}/${_COMPOSER_BUILD_IMG_NAME} || exit 0
        else
          echo "***************************** pull-airflow2 *******************************"
          echo "Environment '"$_COMPOSER_ENV_NAME"' does not require this step ... skipping"
          echo "***************************************************************************"
        fi
    id: 'pull-airflow2'
  # [END pull-airflow2]

# - Build a new Airflow2 image with cache from existing.
  # [START build-airflow2]
  - name: 'gcr.io/cloud-builders/docker'
    waitFor: ['pull-airflow2']
    entrypoint: 'bash'
    args:
      - '-c'
      - |
        if [ "${_COMPOSER_ENV_NAME}" = "${_DEV_ENV_NAME}" ]
        then
            docker build -t gcr.io/${PROJECT_ID}/${_COMPOSER_BUILD_IMG_NAME} \
            --build-arg PYTHON_VERSION=${_PYTHON_VERSION} \
            --cache-from gcr.io/${PROJECT_ID}/${_COMPOSER_BUILD_IMG_NAME} \
            -f docker/Dockerfile-CI .
        else
          echo "***************************** build-airflow2*******************************"
          echo "Environment '"$_COMPOSER_ENV_NAME"' does not require this step ... skipping"
          echo "***************************************************************************"
        fi
    #env:
    #  - 'FOO=$_BAR'
    dir: '.'
    id: 'build-airflow2'
  # [END build-airflow2]

# - Unit tests.
  # [START unit-test]
  - name: 'gcr.io/${PROJECT_ID}/${_COMPOSER_BUILD_IMG_NAME}'
    waitFor: ['build-airflow2']
    entrypoint: 'bash'
    args:
      - '-c'
      - |
        if [ "${_COMPOSER_ENV_NAME}" = "${_DEV_ENV_NAME}" ]
        then
          pytest tests/unit
        else
          echo "***************************** unit-test *******************************"
          echo "Environment '"$_COMPOSER_ENV_NAME"' does not require this step ... skipping"
          echo "***************************************************************************"
        fi
    id: "unit-test"
  # [END unit-test]

# - Integration tests.
  # [START int-test]
  - name: 'gcr.io/${PROJECT_ID}/${_COMPOSER_BUILD_IMG_NAME}'
    waitFor: ['unit-test']
    entrypoint: 'bash'
    args:
      - '-c'
      - |
        if [ "${_COMPOSER_ENV_NAME}" = "${_DEV_ENV_NAME}" ]
        then
          pytest --tc-file ./tests/integration/config.ini -v ./tests/integration
        else
          echo "***************************** integration-test ****************************"
          echo "Environment '"$_COMPOSER_ENV_NAME"' does not require this step ... skipping"
          echo "***************************************************************************"
        fi
    id: "int-test"
  # [END int-test]

# Sync DAGs to 'data' dir for DAG parsing test.
  # [START stage-for-smoke-test]
  - name: 'gcr.io/google.com/cloudsdktool/cloud-sdk'
    waitFor: ['int-test']
    entrypoint: 'gsutil'
    args:
          [
            'rsync','-r', '-d',
            'dags/',
            'gs://${_COMPOSER_INPUT_BUCKET}/data/test-dags/${BUILD_ID}'
          ]
    id: 'stage-for-smoke-test'
  # [END stage-for-smoke-test]

# - Run DAG parsing test.
  # [START dag-parse-smoke-test]
  - name: 'gcr.io/google.com/cloudsdktool/cloud-sdk'
    waitFor: ['stage-for-smoke-test']
    entrypoint: 'bash'
    args:
          [
            'composer_dag_parse.sh',
            '${_COMPOSER_ENV_NAME}',
            '${_COMPOSER_REGION}',
            '${BUILD_ID}'
          ]
    dir: './helpers/scripts'
    id: 'dag-parse-smoke-test'
  # [END dag-parse-smoke-test]

# - Clean up after the smoke test.
  # [START clean-up-after-test]
  - name: 'gcr.io/google.com/cloudsdktool/cloud-sdk'
    waitFor: ['dag-parse-smoke-test']
    entrypoint: 'gsutil'
    args:
          [
            '-m', 'rm','-r',
            'gs://${_COMPOSER_INPUT_BUCKET}/data/test-dags/$BUILD_ID'
          ]
    id: 'clean-up-after-test'
  # [END clean-up-after-test]

# - Style&Lint code.
  # [START lint-code]
  - name: 'gcr.io/${PROJECT_ID}/${_COMPOSER_BUILD_IMG_NAME}'
    waitFor: ['clean-up-after-test']
    entrypoint: 'bash'
    args:
      - '-c'
      - |
        if [ "${_COMPOSER_ENV_NAME}" = "${_DEV_ENV_NAME}" ]
        then
          pip install pre-commit
          pre-commit install
          pre-commit run --all-files
        else
          echo "***************************** Style & Lint code ***************************"
          echo "Environment '"$_COMPOSER_ENV_NAME"' does not require this step ... skipping"
          echo "***************************************************************************"
        fi
    dir: '.'
    id: 'lint-code'
  # [END lint-code]

# Stage composer-airflow-vars.json to data directory.
  # [START stage-airflow-variables]
  - name: 'gcr.io/google.com/cloudsdktool/cloud-sdk'
    waitFor: ['lint-code']
    entrypoint: 'gcloud'
    args:
          [
            'composer', 'environments', 'storage', 'data', 'import',
            '--location=${_COMPOSER_REGION}',
            '--environment=${_COMPOSER_ENV_NAME}',
            '--source','composer-airflow-vars.json',
            '--destination', 'config'
          ]
    dir: './variables'
    id: 'stage-airflow-variables'
  # [END stage-airflow-variables]

# Import Composer Airflow Variables from the file.
  # [START import-airflow-variables]
  - name: 'gcr.io/google.com/cloudsdktool/cloud-sdk'
    waitFor: ['stage-airflow-variables']
    entrypoint: 'gcloud'
    args:
          [
            'beta', 'composer', 'environments', 'run',
            '${_COMPOSER_ENV_NAME}',
            '--location=${_COMPOSER_REGION}',
            'variables', '--',
            'import', '/home/airflow/gcs/data/config/composer-airflow-vars.json'
          ]
    id: 'import-airflow-variables'
  # [END import-airflow-variables]

# - Update Composer env variables.
  # [START update-env-vars]
  - name: 'gcr.io/google.com/cloudsdktool/cloud-sdk'
    waitFor: ['import-airflow-variables']
    entrypoint: 'bash'
    args:
          [
            'composer_vars_update.sh',
            '${_COMPOSER_ENV_NAME}', '${_COMPOSER_REGION}',
            '--update-env-variables', '../../variables/${_COMPOSER_VARS_FILE}'
          ]
    dir: './helpers/scripts'
    id: 'update-env-vars'
  # [END update-env-vars]

# - Update airflow config parameters.
  # [START update-config]
  - name: 'gcr.io/google.com/cloudsdktool/cloud-sdk'
    waitFor: ['update-env-vars']
    entrypoint: 'bash'
    args:
          [
            'composer_conf_update.sh',
            '${_COMPOSER_ENV_NAME}', '${_COMPOSER_REGION}',
            '--update-airflow-configs', '../../ci-cd/${_COMPOSER_CONF_FILE}'
          ]
    dir: './helpers/scripts'
    id: 'update-config'
  # [END update-config]

# - Update PyPi packages.
  # [START update-PyPi]
  - name: 'gcr.io/google.com/cloudsdktool/cloud-sdk'
    waitFor: ['update-config']
    entrypoint: 'bash'
    args:
          [
            'composer_py_update.sh',
            '${_COMPOSER_ENV_NAME}', '${_COMPOSER_REGION}',
            '--update-pypi-packages-from-file', '../../docker/requirements-airflow.txt'
          ]
    dir: './helpers/scripts'
    id: 'update-PyPi'
  # [END update-PyPi]

# - Sync plugins to GCS plugins directory.
  # [START sync-plugins]
  - name: 'gcr.io/google.com/cloudsdktool/cloud-sdk'
    waitFor: ['update-PyPi']
    entrypoint: 'gsutil'
    args:
          [
            'rsync', '-r', '-c', '-d',
            'plugins/',
            'gs://${_COMPOSER_INPUT_BUCKET}/plugins'
          ]
    id: 'sync-plugins'
  # [END sync-plugins]

# - Sync DAGs to GCS DAGs directory.
  # [START sync-dags]
  - name: 'gcr.io/google.com/cloudsdktool/cloud-sdk'
    waitFor: ['sync-plugins']
    entrypoint: 'gsutil'
    args:
          [
            'rsync', '-r', '-c', '-d',
            'dags/',
            'gs://${_COMPOSER_INPUT_BUCKET}/dags'
          ]
    id: 'sync-dags'
  # [END sync-dags]

# - Run a test DAG & send email
  # [START send-email]
  - name: 'gcr.io/google.com/cloudsdktool/cloud-sdk'
    waitFor: ['sync-dags']
    entrypoint: 'bash'
    args:
          [
            'composer_dag_send_email.sh',
            '${_COMPOSER_ENV_NAME}',
            '${_COMPOSER_REGION}',
            '${_DAG_TO_EMAIL}', '6', '20',
            '${SHORT_SHA}'
          ]
    dir: './helpers/scripts'
    id: 'send-email'
 # [END send-email]

timeout: 3600s
images:
  - 'gcr.io/${PROJECT_ID}/${_COMPOSER_BUILD_IMG_NAME}'
